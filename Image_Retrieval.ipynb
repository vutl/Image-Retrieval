{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n6ssqMGy8to3",
        "Bsho9Rp364Rx",
        "6ZNw-NOzxQ80",
        "nzMkh1jgyA3Z",
        "pB8wjxcBzR9f",
        "XmbLodqx3sQ4",
        "qmNSEGEK87Ij",
        "wuhg09OW9AiS",
        "zTCOxRSv9XUN",
        "pV4kv5729iZv"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRATMAVTq2xmPwIvEmgAp7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vutl/Image-Retrieval/blob/feature%2Fimg-retrieval/Image_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Truy vấn hình ảnh cơ bản**"
      ],
      "metadata": {
        "id": "n6ssqMGy8to3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Xây dựng chương trình truy vấn ảnh cơ bản.\n",
        "*   Phát triển chương trình truy vấn ảnh nâng cao với CLIP model và vector database.\n",
        "*   (Optional) Thu thập và xử lý dữ liệu nhằm mục đích xây dựng chương trình truy vấn ảnh cá nhân\n",
        "hóa.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VLGxqpfA6XwS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NpF_nE2J3F7N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ec_6koew806r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lấy danh sách các class của ảnh trong data"
      ],
      "metadata": {
        "id": "-UDgUQnz6TNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = 'data'\n",
        "CLASS_NAME = sorted(list(os.listdir(f'{ROOT}/train')))"
      ],
      "metadata": {
        "id": "8qEyt5jZ3iqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "đọc ảnh, resize về kích thước chung (thì\n",
        "mới áp dụng được các phép đo) và chuyển đổi nó về dạng numpy:"
      ],
      "metadata": {
        "id": "Ax-DldrI6OmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image_from_path(path, size):\n",
        "  im = Image.open(path).convert('RGB').resize(size)\n",
        "  return np.array(im)\n",
        "\n",
        "def folder_to_images(folder, size):\n",
        "  list_dir = [folder + '/' + name for name in os.listdir(folder)]\n",
        "  images_np = np.zeros(shape=(len(list_dir) *size, 3))\n",
        "  images_path = []\n",
        "  for i, path in enumerate(list_dir):\n",
        "    images_np[i] = read_image_from_path(path, size)\n",
        "    images_path.append(path)\n",
        "  images_path = np.array(images_path)\n",
        "  return images_np, images_path"
      ],
      "metadata": {
        "id": "tkygDmPZ4rDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Truy vấn hình ảnh với độ đo L1**"
      ],
      "metadata": {
        "id": "Bsho9Rp364Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def absolute_difference(query, data):\n",
        "  axis_batch_size = tuple(range(1, len(data.shape)))\n",
        "  return np.sum(np.abs(query - data), axis=axis_batch_size)"
      ],
      "metadata": {
        "id": "uag73GyG634J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tính toán để tính độ tương đồng giữa ảnh input và các hình ảnh trong\n",
        "bộ dữ liệu. hàm ***get_l1_score*** sẽ trả về ảnh ***query*** và ***ls_path_score*** chứa\n",
        "danh sách hình ảnh và giá trị độ tương đồng với từng ảnh."
      ],
      "metadata": {
        "id": "wCJYHY1bwLxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_l1_score(root_img_path, query_path, size):\n",
        "  query = read_image_from_path(query_path, size)\n",
        "  ls_path_score = []\n",
        "  for folder in os.listdir(root_img_path):\n",
        "    if folder in CLASS_NAME:\n",
        "      path = root_img_path + folder\n",
        "      images_np, images_path = folder_to_images(path, size) # mang numpy nhieu anh, paths\n",
        "      rates = absolute_difference(query, images_np)\n",
        "      ls_path_score.extend(list(zip(images_path, rates)))\n",
        "  return query, ls_path_score"
      ],
      "metadata": {
        "id": "g5xNoWdD8Mn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đoạn code này thực hiện quá trình truy xuất hình ảnh bằng cách so sánh một hình ảnh truy vấn với\n",
        "các hình ảnh trong tập huấn luyện dựa trên điểm L1. Đầu tiên, các hình ảnh được thay đổi cùng kích\n",
        "thước. Tiếp theo hệ thống sẽ so sánh ảnh truy vấn với các hình ảnh trong thư mục huấn luyện để tính\n",
        "điểm L1. Sau đó, kết quả truy vấn được trả về là danh sách các đường dẫn chứa hình ảnh và điểm số\n",
        "tính theo L1. Cuối cùng 5 kết quả tốt nhất sẽ được hiển thị cùng với ảnh truy vấn"
      ],
      "metadata": {
        "id": "Bpz2t1FzwlIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_img_path = f\"{ROOT}/train/\"\n",
        "query_path = f\"{ROOT}/test/Orange_easy/0_100.jpg\"\n",
        "size = (448, 448)\n",
        "query, ls_path_score = get_l1_score(root_img_path, query_path, size)\n",
        "plot_results(query_path, ls_path_score, reverse=False)"
      ],
      "metadata": {
        "id": "0zKsITk6vSjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_img_path = f\"{ROOT}/train/\"\n",
        "query_path = f\"{ROOT}/test/African_crocodile/n01697457_18534.JPEG\"\n",
        "size = (448, 448)\n",
        "query, ls_path_score = get_l1_score(root_img_path, query_path, size)\n",
        "plot_results(query_path, ls_path_score, reverse=False)"
      ],
      "metadata": {
        "id": "poTRFetswysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Truy vấn hình ảnh với độ đo L2**"
      ],
      "metadata": {
        "id": "6ZNw-NOzxQ80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_square_difference(query, data):\n",
        "  axis_batch_size = tuple(range(1, len(data.shape)))\n",
        "  return np.mean((data - query)**2, axis=axis_batch_size)"
      ],
      "metadata": {
        "id": "j3D4RsPLxSgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_l2_score(root_img_path, query_path, size):\n",
        "  query = read_image_from_path(query_path, size)\n",
        "  ls_path_score = []\n",
        "  for folder in os.listdir(root_img_path):\n",
        "    if folder in CLASS_NAME:\n",
        "      path = root_img_path + folder\n",
        "      images_np, images_path = folder_to_images(path, size) # mang numpy nhieu anh, paths\n",
        "      rates = mean_square_difference(query, images_np)\n",
        "      ls_path_score.extend(list(zip(images_path, rates)))\n",
        "  return query, ls_path_score"
      ],
      "metadata": {
        "id": "KpEWOsB0xfE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_img_path = f\"{ROOT}/train/\"\n",
        "query_path = f\"{ROOT}/test/Orange_easy/0_100.jpg\"\n",
        "size = (448, 448)\n",
        "query, ls_path_score = get_l2_score(root_img_path, query_path, size)\n",
        "plot_results(query_path, ls_path_score, reverse=False)"
      ],
      "metadata": {
        "id": "WAcZSUv2x2Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_img_path = f\"{ROOT}/train/\"\n",
        "query_path = f\"{ROOT}/test/African_crocodile/n01697457_18534.JPEG\"\n",
        "size = (448, 448)\n",
        "query, ls_path_score = get_l2_score(root_img_path, query_path, size)\n",
        "plot_results(query_path, ls_path_score, reverse=False)"
      ],
      "metadata": {
        "id": "MwDr7QPqx2oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Truy vấn hình ảnh với độ đo Cosine Similarity**"
      ],
      "metadata": {
        "id": "nzMkh1jgyA3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(query, data):\n",
        "  axis_batch_size = tuple(range(1, len(data.shape)))\n",
        "  query_norm = np.sqrt(np.sum(query**2))\n",
        "  data_norm = np.sqrt(np.sum(data**2, axis=axis_batch_size))\n",
        "  return np.sum(data * query, axis=axis_batch_size) / (query_norm * data_norm + np.finfo(float).eps)"
      ],
      "metadata": {
        "id": "JZYYVXLEyDK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cosine_similarity_score(root_img_path, query_path, size):\n",
        "  query = read_image_from_path(query_path, size)\n",
        "  ls_path_score = []\n",
        "  for folder in os.listdir(root_img_path):\n",
        "    if folder in CLASS_NAME:\n",
        "      path = root_img_path + folder\n",
        "      images_np, images_path = folder_to_images(path, size) # mang numpy nhieu anh, paths\n",
        "      rates = cosine_similarity(query, images_np)\n",
        "      ls_path_score.extend(list(zip(images_path, rates)))\n",
        "  return query, ls_path_score"
      ],
      "metadata": {
        "id": "cbFnHfgryshZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Để hiển thị kết quả chúng ta sử dụng hàm plot_results(), tuy nhiên ở hàm này chúng ta sẽ sắp xếp giá\n",
        "trị giảm dần từ lớn đến nhỏ vì với độ đo này thì giá trị càng lớn sẽ càng giống nhau, cho nên chúng ta\n",
        "sử dụng reverse = True."
      ],
      "metadata": {
        "id": "dkvWn7DbzBui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_img_path = f\"{ROOT}/train/\"\n",
        "query_path = f\"{ROOT}/test/Orange_easy/0_100.jpg\"\n",
        "size = (448, 448)\n",
        "query, ls_path_score = get_cosine_similarity_score(root_img_path, query_path, size)\n",
        "plot_results(query_path, ls_path_score, reverse=False)"
      ],
      "metadata": {
        "id": "z0NioKZazDCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_img_path = f\"{ROOT}/train/\"\n",
        "query_path = f\"{ROOT}/test/African_crocodile/n01697457_18534.JPEG\"\n",
        "size = (448, 448)\n",
        "query, ls_path_score = get_cosine_similarity_score(root_img_path, query_path, size)\n",
        "plot_results(query_path, ls_path_score, reverse=False)"
      ],
      "metadata": {
        "id": "mwrZvZjjzFkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Truy vấn hình ảnh với độ đo Correlation Coefficient**"
      ],
      "metadata": {
        "id": "pB8wjxcBzR9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_coefficient(query, data):\n",
        "  axis_batch_size = tuple(range(1, len(data.shape)))\n",
        "  query_mean = query - np.mean(query)\n",
        "  data_mean = data - np.mean(data, axis=axis_batch_size, keepdims=True)\n",
        "  query_norm = np.sqrt(np.sum(query_mean**2))\n",
        "  data_norm = np.sqrt(np.sum(data_mean**2, axis=axis_batch_size))\n",
        "  return np.sum(data_mean * query_mean, axis=axis_batch_size) / (query_norm * data_norm + np.finfo(float).eps)"
      ],
      "metadata": {
        "id": "iMgkC7_tzUjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correlation_coefficient_score(root_img_path, query_path, size):\n",
        "  query = read_image_from_path(query_path, size)\n",
        "  ls_path_score = []\n",
        "  for folder in os.listdir(root_img_path):\n",
        "    if folder in CLASS_NAME:\n",
        "      path = root_img_path + folder\n",
        "      images_np, images_path = folder_to_images(path, size) # mang numpy nhieu anh, paths\n",
        "      rates = correlation_coefficient(query, images_np)\n",
        "      ls_path_score.extend(list(zip(images_path, rates)))\n",
        "  return query, ls_path_score"
      ],
      "metadata": {
        "id": "GoAsSVvWzzED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Để hiển thị kết quả chúng ta sử dụng hàm plot_results(), tuy nhiên ở hàm này chúng ta sẽ sắp xếp giá\n",
        "trị giảm dần từ lớn đến nhỏ vì với độ đo này thì giá trị càng lớn sẽ càng giống nhau, cho nên chúng ta\n",
        "sử dụng reverse = True"
      ],
      "metadata": {
        "id": "gqKhbvyd0F6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_img_path = f\"{ROOT}/train/\"\n",
        "query_path = f\"{ROOT}/test/Orange_easy/0_100.jpg\"\n",
        "size = (448, 448)\n",
        "query, ls_path_score = get_correlation_coefficient_score(root_img_path, query_path, size)\n",
        "plot_results(query_path, ls_path_score, reverse=True)"
      ],
      "metadata": {
        "id": "I-jYmrBX0Kt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_img_path = f\"{ROOT}/train/\"\n",
        "query_path = f\"{ROOT}/test/African_crocodile/n01697457_18534.JPEG\"\n",
        "size = (448, 448)\n",
        "query, ls_path_score = get_correlation_coefficient_score(root_img_path, query_path, size)\n",
        "plot_results(query_path, ls_path_score, reverse=True)"
      ],
      "metadata": {
        "id": "AnW7S7s90NJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Truy vấn hình ảnh nâng cao với Pretrained Deep Learning Model**"
      ],
      "metadata": {
        "id": "XmbLodqx3sQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Thư viện chromadb hỗ trợ việc quản lý và truy xuất dữ liệu hình ảnh hiệu quả (sử dụng thêm với mục đích tạo vector\n",
        "database)\n",
        "*   chromadb có thể dùng open-clip-torch để cung cấp khả năng sử dụng mô hình CLIP đã\n",
        "được đào tạo sẵn, đây là một công cụ mạnh mẽ để phân tích nội dung hình ảnh thông qua học sâu."
      ],
      "metadata": {
        "id": "kngaEUOo4W9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install open-clip-torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbWutZxb3zbh",
        "outputId": "f19ba2ba-c021-4f05-b029-b6c7e4489f16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.8.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.112.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.30.5-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.5)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.27.0 (from chromadb)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting importlib-metadata<=8.0.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.23.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Downloading chromadb-0.5.5-py3-none-any.whl (584 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.112.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.47b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.5-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.7/427.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=a50324a6a0806a607807510ee893d24fb0b2340a7d51e7584798ee6ab5de605e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, python-dotenv, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, httpcore, coloredlogs, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, kubernetes, httpx, fastapi, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.2.0\n",
            "    Uninstalling importlib_metadata-8.2.0:\n",
            "      Successfully uninstalled importlib_metadata-8.2.0\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 chroma-hnswlib-0.7.6 chromadb-0.5.5 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.112.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 humanfriendly-10.0 importlib-metadata-8.0.0 kubernetes-30.1.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.18.1 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-instrumentation-0.47b0 opentelemetry-instrumentation-asgi-0.47b0 opentelemetry-instrumentation-fastapi-0.47b0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-util-http-0.47b0 orjson-3.10.7 overrides-7.7.0 posthog-3.5.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.37.2 uvicorn-0.30.5 uvloop-0.19.0 watchfiles-0.23.0 websockets-12.0\n",
            "Collecting open-clip-torch\n",
            "  Downloading open_clip_torch-2.26.1-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.18.1+cu121)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2024.5.15)\n",
            "Collecting ftfy (from open-clip-torch)\n",
            "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (4.66.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.23.5)\n",
            "Collecting timm (from open-clip-torch)\n",
            "  Downloading timm-1.0.8-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.0->open-clip-torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->open-clip-torch) (0.2.13)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (2.32.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->open-clip-torch) (0.4.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->open-clip-torch) (1.3.0)\n",
            "Downloading open_clip_torch-2.26.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m96.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-1.0.8-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm, open-clip-torch\n",
            "Successfully installed ftfy-6.2.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 open-clip-torch-2.26.1 timm-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction"
      ],
      "metadata": {
        "id": "ZfffgDXP4A8I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tương tự như các bước ở phần cơ bản trước, nhưng chúng ta sẽ nâng cấp bằng cách thêm một hàm để trích xuất vector\n",
        "đặc trưng cho mỗi hình ảnh. Mô hình CLIP sẽ được sử dụng để biến đổi hình ảnh thành các vector đặc\n",
        "trưng đại diện cho nội dung và ngữ cảnh của hình ảnh đó. Sau đó, việc so sánh các hình ảnh không\n",
        "được thực hiện trực tiếp trên ảnh gốc mà là thông qua việc tính sự tương đồng giữa các vector này.\n",
        "Đoạn code bên đưới khởi tạo một hàm để trích xuất vector đặc trưng từ một hình sử dụng mô hình CLIP.\n",
        "Tiếp theo, hàm get_single_image_embedding nhận một hình ảnh làm đầu vào và sử dụng phương thức\n",
        "_encode_image của OpenCLIPEmbeddingFunction để trích xuất ảnh thành một vector đặc trưng."
      ],
      "metadata": {
        "id": "gtkv4HKD42nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_function = OpenCLIPEmbeddingFunction()\n",
        "\n",
        "def get_single_image_embedding(image):\n",
        "  embedding = embedding_function._encode_image(image=image)\n",
        "  return np.array(embedding)"
      ],
      "metadata": {
        "id": "5b6qCNYE4Osy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Truy vấn embedding vector với độ đo L1**"
      ],
      "metadata": {
        "id": "qmNSEGEK87Ij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Truy vấn embedding vector với độ đo L1 hàm ***get_l1_score*** được nâng cấp lên bằng cách sử dụng\n",
        "CLIP model để trích xuất vector đặc trưng"
      ],
      "metadata": {
        "id": "9MbnZq235bYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_l1_score(root_img_path, query_path, size):\n",
        "  query = read_image_from_path(query_path, size)\n",
        "  query_embedding = get_single_image_embedding(query)\n",
        "  ls_path_score = []\n",
        "  for folder in os.listdir(root_img_path):\n",
        "    if folder in CLASS_NAME:\n",
        "      path = root_img_path + folder\n",
        "      images_np, images_path = folder_to_images(path, size) # mang numpy nhieu anh, paths\n",
        "      embedding_list = []\n",
        "      for idx_img in range(images_np.shape[0]):\n",
        "        embedding = get_single_image_embedding(images_np[idx_img].astype(np.unit8))\n",
        "        embedding_list.append(embedding)\n",
        "      rates = absolute_difference(query_embedding, np.stack(embedding_list))\n",
        "      ls_path_score.extend(list(zip(images_path, rates)))\n",
        "  return query, ls_path_score"
      ],
      "metadata": {
        "id": "soy-Pb0y5Yvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Truy vấn embedding vector với độ đo L2**"
      ],
      "metadata": {
        "id": "wuhg09OW9AiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_l2_score(root_img_path, query_path, size):\n",
        "  query = read_image_from_path(query_path, size)\n",
        "  query_embedding = get_single_image_embedding(query)\n",
        "  ls_path_score = []\n",
        "  for folder in os.listdir(root_img_path):\n",
        "    if folder in CLASS_NAME:\n",
        "      path = root_img_path + folder\n",
        "      images_np, images_path = folder_to_images(path, size) # mang numpy nhieu anh, paths\n",
        "      embedding_list = []\n",
        "      for idx_img in range(images_np.shape[0]):\n",
        "        embedding = get_single_image_embedding(images_np[idx_img].astype(np.unit8))\n",
        "        embedding_list.append(embedding)\n",
        "      rates = mean_square_difference(query_embedding, np.stack(embedding_list))\n",
        "      ls_path_score.extend(list(zip(images_path, rates)))\n",
        "  return query, ls_path_score"
      ],
      "metadata": {
        "id": "RtcLo4T19BRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Truy vấn embedding vector với độ đo Cosine Similarity**"
      ],
      "metadata": {
        "id": "zTCOxRSv9XUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cosine_similarity_score(root_img_path, query_path, size):\n",
        "  query = read_image_from_path(query_path, size)\n",
        "  query_embedding = get_single_image_embedding(query)\n",
        "  ls_path_score = []\n",
        "  for folder in os.listdir(root_img_path):\n",
        "    if folder in CLASS_NAME:\n",
        "      path = root_img_path + folder\n",
        "      images_np, images_path = folder_to_images(path, size) # mang numpy nhieu anh, paths\n",
        "      embedding_list = []\n",
        "      for idx_img in range(images_np.shape[0]):\n",
        "        embedding = get_single_image_embedding(images_np[idx_img].astype(np.unit8))\n",
        "        embedding_list.append(embedding)\n",
        "      rates = cosine_similarity(query_embedding, np.stack(embedding_list))\n",
        "      ls_path_score.extend(list(zip(images_path, rates)))\n",
        "  return query, ls_path_score"
      ],
      "metadata": {
        "id": "x0xXUFGT9Z0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Truy vấn embedding vector với độ đo Correlation Coefficent**"
      ],
      "metadata": {
        "id": "pV4kv5729iZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correlation_coefficient_score(root_img_path, query_path, size):\n",
        "  query = read_image_from_path(query_path, size)\n",
        "  query_embedding = get_single_image_embedding(query)\n",
        "  ls_path_score = []\n",
        "  for folder in os.listdir(root_img_path):\n",
        "    if folder in CLASS_NAME:\n",
        "      path = root_img_path + folder\n",
        "      images_np, images_path = folder_to_images(path, size) # mang numpy nhieu anh, paths\n",
        "      embedding_list = []\n",
        "      for idx_img in range(images_np.shape[0]):\n",
        "        embedding = get_single_image_embedding(images_np[idx_img].astype(np.unit8))\n",
        "        embedding_list.append(embedding)\n",
        "      rates = correlation_coefficient(query_embedding, np.stack(embedding_list))\n",
        "      ls_path_score.extend(list(zip(images_path, rates)))\n",
        "  return query, ls_path_score"
      ],
      "metadata": {
        "id": "nkIdwuWn9p6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tối ưu hoá quá trình truy vấn hình ảnh sử dụng mô hình CLIP và cơ sở dữ liệu vector**"
      ],
      "metadata": {
        "id": "gmNUzdji-BoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phương pháp này sẽ sử dụng một cơ sở dữ liệu vector (vector database) để quản lý các\n",
        "embedding vector, giúp quá trình truy vấn được tối ưu hơn"
      ],
      "metadata": {
        "id": "ig-kYxQW-dyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_files_path(path):\n",
        "  files_path = []\n",
        "  for label in CLASS_NAME:\n",
        "    label_path = path + \"/\" + label\n",
        "    filenames = os.listdir(label_path)\n",
        "    for filename in filenames:\n",
        "      filepath = label_path + '/' + filename\n",
        "      files_path.append(filepath)\n",
        "  return files_path\n",
        "\n",
        "data_path = f'{ROOT}/train'\n",
        "files_path = get_files_path(data_path)"
      ],
      "metadata": {
        "id": "EM0lzMuU-eRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Truy vấn ảnh với L2 Collection**"
      ],
      "metadata": {
        "id": "MSrEETnaEjh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm giúp trích xuất và lưu trữ các vector\n",
        "đặc trưng của ảnh vào một collection đã được tạo (collection -  tập hợp các\n",
        "vector hoặc tài liệu được chỉ mục và lưu trữ cùng nhau dựa trên một số tiêu chí hoặc đặc điểm chung, dùng\n",
        "để tổ chức và quản lý dữ liệu)"
      ],
      "metadata": {
        "id": "TVd-TWUuAOl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_embedding(collection, files_path):\n",
        "  ids = []\n",
        "  embeddings = []\n",
        "  for id_filepath, filepath in tqdm(enumerate(files_path)):\n",
        "    ids.append(f'id_{id_filepath}')\n",
        "    image = Image.open(filepath)\n",
        "    embedding = get_single_image_embedding(image=image)\n",
        "    embeddings.append(embedding)\n",
        "  collection.add(\n",
        "      embeddings=embeddings,\n",
        "      ids=ids\n",
        "  )"
      ],
      "metadata": {
        "id": "gumbVnLRAYlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Khởi tạo một client cho cơ sở dữ liệu Chroma và tạo một collection mới với cấu hình\n",
        "sử dụng L2 để so sánh các embedding vector. Sau đó, gọi hàm add_embedding để thêm các vector đặc trưng của ảnh vào collection này, qua đó tạo điều kiện thuận lợi cho việc truy vấn nhanh chóng và hiệu quả"
      ],
      "metadata": {
        "id": "dw0JIwQbCsoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a Chroma Client\n",
        "chroma_client - chromadb.Client()\n",
        "#Create a collection\n",
        "l2_collection = chroma_client.get_or_create_collection(name='l2_collection',\n",
        "                                                      metadata={HNSW_SPACE: \"l2\"})\n",
        "add_embedding(collection=l2_collection, files_path=files_path)"
      ],
      "metadata": {
        "id": "twFm44OEC1qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm search được định nghĩa để thực hiện truy xuất các ảnh dựa trên embedding của ảnh truy vấn.\n",
        "Hàm này nhận đường dẫn của ảnh truy vấn, loại collection và số lượng kết quả trả về mong muốn, sau\n",
        "đó trả về danh sách các kết quả phù hợ"
      ],
      "metadata": {
        "id": "XGVNtVu-DTBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search(image_path, collection, n_results):\n",
        "  query_image = Image.open(image_path)\n",
        "  query_embedding = get_single_image_embedding(query_image)\n",
        "  results = collection.query(\n",
        "      query_embeddings=query_embedding,\n",
        "      n_results=n_results #how many results to return\n",
        "  )\n",
        "  return resutls"
      ],
      "metadata": {
        "id": "xrAoOqhaDWUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = f'{ROOT}/test'\n",
        "test_files_path = get_files_path(path=test_path)\n",
        "test_results = test_files_path[1]\n",
        "l2_results = search(image_path=test_path, collection=l2_collection, n_results=5)\n",
        "plot_results(image_path=test_path, files_path=files_path, results=12_results)"
      ],
      "metadata": {
        "id": "deIXif1RD6Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Truy vấn ảnh với Cosine Similarity Collection**"
      ],
      "metadata": {
        "id": "o79Pc_4eEu4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a collection\n",
        "cosine_similarity = chroma.get_or_create_collection(name='cosine_similarity',\n",
        "                                                      metadata={HNSW_SPACE: \"cosine\"})\n",
        "add_embedding(collection=cosine_collection, files_path=files_path)"
      ],
      "metadata": {
        "id": "1mx50RKXExcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chương trình Truy Vấn Ảnh Cá Nhân Hóa**"
      ],
      "metadata": {
        "id": "t1r6Q7yEIHFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "!apt-get update\n",
        "!apt-get install -y wget\n",
        "!pip install selenium\n",
        "!apt-get install -y chromium-browser\n",
        "!apt-get install -y chromium-chromedriver\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "from bs4 import BeautifulSoup # For parsing HTML content\n",
        "from urllib.parse import urljoin, urlparse # For handling URLs\n",
        "import urllib.request # For making HTTP requests\n",
        "import time # For handling time-related operations\n",
        "import os # For interacting with the operating system (relate to dir, folder, file)\n",
        "from tqdm import tqdm # For displaying progress bars (visualize progress)\n",
        "import concurrent.futures # For multi-threading\n",
        "import json # For writing to a text file\n",
        "from PIL import Image # For handling images\n",
        "from posix import terminal_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlkU1RXZIVFj",
        "outputId": "677c31d9-a152-48f6-e0db-424589bbed0d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [921 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,552 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,841 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,218 kB]\n",
            "Hit:16 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,152 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.8 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,422 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,954 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,441 kB]\n",
            "Fetched 23.8 MB in 3s (7,602 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.23.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.23.1-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.23.1 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 44 not upgraded.\n",
            "Need to get 28.4 MB of archives.\n",
            "After this operation, 117 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3 [595 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.63+22.04ubuntu0.1 [25.9 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 28.4 MB in 3s (10.2 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123594 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.3_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123802 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.63+22.04ubuntu0.1_amd64.deb ...\n",
            "Unpacking snapd (2.63+22.04ubuntu0.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.63+22.04ubuntu0.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124032 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  chromium-chromedriver\n",
            "0 upgraded, 1 newly installed, 0 to remove and 44 not upgraded.\n",
            "Need to get 2,308 B of archives.\n",
            "After this operation, 77.8 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Fetched 2,308 B in 0s (16.4 kB/s)\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "(Reading database ... 124053 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thu thập dữ liệu - Crawl URL từ Website**"
      ],
      "metadata": {
        "id": "LILku1BUTs4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UrlScraper:\n",
        "  #Constructor\n",
        "  def __init__(self, url_template, max_images=50, max_workers=4):\n",
        "    self.url_template = url_template #link crawl\n",
        "    self.max_images = max_images #Max images\n",
        "    self.max_workers = max_workers #Thread\n",
        "    self.setup_environment() #Call for set up environment\n",
        "\n",
        "  #Set up environment\n",
        "  def setup_environment(self):\n",
        "    os.environ['PATH'] += ':usr/lib/chronium-browser'\n",
        "    os.environ['PATH'] += ':usr/lib/chronium-browser/chromedriver'\n",
        "\n",
        "  def get_url_images(self, term):\n",
        "    \"\"\"\n",
        "    Crawl the urls of images by term\n",
        "\n",
        "    Parameters:\n",
        "    term (str): The name of animal, plant, scenery, furniture\n",
        "\n",
        "    Returns:\n",
        "    urls (list): List of urls of images\n",
        "    \"\"\"\n",
        "\n",
        "    #Initialize Chrome driver\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    url = self.url_template.format(search_term=term)\n",
        "    driver.get(url)\n",
        "\n",
        "    # Start crawl urls of image like brute force - the same mechanism with this but add some feature\n",
        "    urls = []\n",
        "    more_content_available = True\n",
        "\n",
        "    pbar = tqdm(total=self.max_images, desc=f\"Fetching images for {term}\") #Set up for visualize progress\n",
        "\n",
        "    while len(urls) < self.max_images and more_content_available:\n",
        "      soup  = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "      img_tags = soup.find_all(\"img\")\n",
        "\n",
        "      for img in img_tags:\n",
        "        if len(urls) >= self.max_images:\n",
        "          break\n",
        "        if 'src' in img.attrs:\n",
        "          href = img.attrs['src']\n",
        "          img_path = urljoin(url, href)\n",
        "          img_path = img_path.replace(\"_m.jpg\", \"_b.jpg\").replace(\"_n.jpg\", \"_b.jpg\").replace(\"_w.jpg\", \"_b.jpg\")\n",
        "          if img_path ==  \"https://combo.staticflickr.com/ap/build/images/getty/IStock_corporate_logo.svg\":\n",
        "            continue\n",
        "          urls.append(img_path)\n",
        "          pbar.update(1)\n",
        "\n",
        "      try:\n",
        "        load_more_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//button[@id=\"yui_3_16_0_1_1721642285931_28620\"]')))\n",
        "        load_more_button.click()\n",
        "        time.sleep(2)\n",
        "      except:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(2)\n",
        "\n",
        "        new_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        new_img_tags = new_soup.find_all(\"img\", loading_=\"lazy\")\n",
        "        if len(new_img_tags) == len(img_tags):\n",
        "          more_content_available = False\n",
        "        img_tags = new_img_tags\n",
        "\n",
        "    pbar.close()\n",
        "    driver.quit()\n",
        "    return urls\n",
        "\n",
        "  def scrape_urls(self, categories):\n",
        "    \"\"\"\n",
        "    Call get_url_images method to get all urls of any object in categories\\\n",
        "\n",
        "    Parameter:\n",
        "    categories (dictionary): the dict of all object we need to collect image with format\n",
        "      categories{\"name_object\": [value1, value2, ...]}\n",
        "\n",
        "    Returns:\n",
        "    all_urls (dictionary): Dictionary of urls of images\n",
        "    \"\"\"\n",
        "\n",
        "    all_urls = {category: {} for category in categories}\n",
        "\n",
        "    #Handle multi-threading for efficient installation\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "      futures_to_term = {executor.submit(self.get_url_images, term): (category, term) for category, terms in categories.items() for term in terms}\n",
        "\n",
        "      for future in tqdm(concurrent.futures.as_completed(futures_to_term), total=len(futures_to_term), desc=\"Overall Progress\"):\n",
        "        category, term = futures_to_term[future]\n",
        "\n",
        "        try:\n",
        "          urls = future.result()\n",
        "          all_urls[category][term] = urls\n",
        "          print(f\"\\nNumber of images retrieved for {term}: {len(urls)}\")\n",
        "        except Exception as exc:\n",
        "          print(f\"\\n{term} generated an exception: {exc}\")\n",
        "\n",
        "    return all_urls\n",
        "\n",
        "  def save_to_file(self, data,  filename):\n",
        "    \"\"\"\n",
        "    Save the data to a JSON file.\n",
        "\n",
        "    Parameters:\n",
        "    data (dict): The data to be saved.\n",
        "    filename (str): The name of the JSON file.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "\n",
        "    \"\"\"\n",
        "    with open(filename, \"w\") as file:\n",
        "      json.dump(data, file, indent=4)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "\n",
        "categories = {\n",
        "    \"animal\": [\"Monkey\", \"Elephant\", \"cows\", \"Cat\", \"Dog\", \"bear\", \"fox\", \"Civet\", \"Pangolins\",\n",
        "               \"Rabbit\", \"Bats\", \"Whale\", \"Cock\", \"Owl\", \"flamingo\", \"Lizard\", \"Turtle\", \"Snake\",\n",
        "               \"Frog\", \"Fish\", \"shrimp\", \"Crab\", \"Snail\", \"Coral\", \"Jellyfish\", \"Butterfly\", \"Flies\",\n",
        "               \"Mosquito\", \"Ants\", \"Cockroaches\", \"Spider\", \"scorpion\", \"tiger\", \"bird\", \"horse\",\n",
        "               \"pig\", \"Alligator\", \"Alpaca\", \"Anteater\", \"donkey\", \"Bee\", \"Buffalo\", \"Camel\"],\n",
        "\n",
        "    \"plant\": [\"Bamboo\", \"Apple\", \"Apricot\", \"Banana\", \"Bean\", \"Wildflower\", \"Flower\",\n",
        "              \"Mushroom\", \"Weed\", \"Fern\", \"Reed\", \"Shrub\", \"Moss\", \"Grass\", \"Palmtree\", \"Corn\",\n",
        "              \"Tulip\", \"Rose\", \"Clove\", \"Dogwood\", \"Durian\", \"Ferns\", \"Fig\", \"Flax\", \"Frangipani\",\n",
        "              \"Lantana\", \"Hibiscus\", \"Bougainvillea\", \"Pea\", \"OrchidTree\", \"RangoonCreeper\",\n",
        "              \"Jackfruit\", \"Cottonplant\", \"Cornelainetree\", \"Coffeplant\", \"Coconut\", \"wheat\",\n",
        "              \"watermelon\", \"radish\", \"carrot\"],\n",
        "\n",
        "    \"furniture\": [\"bed\", \"cabinet\", \"chair\", \"chests\", \"clock\", \"desks\", \"table\", \"Piano\",\n",
        "                  \"Bookcase\", \"Umbrella\", \"Clothes\", \"cart\", \"sofa\", \"ball\", \"spoon\", \"Bowl\", \"fridge\",\n",
        "                  \"pan\", \"book\"],\n",
        "\n",
        "    \"scenery\": [\"Cliff\", \"Bay\", \"Coast\", \"Mountains\", \"Forests\", \"Waterbodies\", \"Lake\",\n",
        "                \"desert\", \"Farmland\", \"river\", \"hedges\", \"plain\", \"sky\", \"cave\", \"cloud\", \"flowergarden\",\n",
        "                \"glacier\", \"grassland\", \"horizon\", \"lighthouse\", \"plateau\", \"savannah\", \"valley\",\n",
        "                \"volcano\", \"waterfall\"]\n",
        "}\n",
        "\n",
        "urltopic = {\"flickr\": \"https://www.flickr.com/search/?text={search_term}\"}\n",
        "scraper = UrlScraper(url_template=urltopic[\"flickr\"], max_images=20, max_workers=5)\n",
        "image_urls = scraper.scrape_urls(categories)\n",
        "scraper.save_to_file(image_urls, 'image_urls.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3TrXB3cKRTI",
        "outputId": "47412c9c-3871-42a3-e8e6-4a7a8aa3ded6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Overall Progress:   0%|          | 0/127 [00:00<?, ?it/s]\n",
            "Fetching images for cows:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for cows:   5%|▌         | 1/20 [00:01<00:20,  1.09s/it]\u001b[A\n",
            "\n",
            "Fetching images for Dog:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Dog:   5%|▌         | 1/20 [00:02<00:43,  2.26s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Monkey:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Elephant:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Cat:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Monkey:   5%|▌         | 1/20 [00:01<00:30,  1.62s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Elephant:   5%|▌         | 1/20 [00:01<00:34,  1.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for cows: 100%|██████████| 20/20 [00:14<00:00,  1.42it/s]\n",
            "Overall Progress:   1%|          | 1/127 [00:43<1:32:21, 43.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for cows: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for bear:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for Dog: 100%|██████████| 20/20 [00:14<00:00,  1.34it/s]\n",
            "Overall Progress:   2%|▏         | 2/127 [00:48<43:02, 20.66s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Dog: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for fox:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Monkey: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n",
            "Overall Progress:   2%|▏         | 3/127 [00:52<26:51, 13.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Monkey: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Elephant: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
            "Fetching images for Cat: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Elephant: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\rOverall Progress:   4%|▍         | 5/127 [00:54<11:19,  5.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Cat: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "Fetching images for bear: 100%|██████████| 20/20 [00:17<00:00,  1.16it/s]\n",
            "\n",
            "\n",
            "\n",
            "Overall Progress:   5%|▍         | 6/127 [01:04<14:34,  7.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for bear: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Pangolins:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Rabbit:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Fetching images for Pangolins:   5%|▌         | 1/20 [00:02<00:51,  2.72s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for fox: 100%|██████████| 20/20 [00:20<00:00,  1.01s/it]\n",
            "Overall Progress:   6%|▌         | 7/127 [01:11<14:27,  7.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for fox: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for Bats:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Bats:   5%|▌         | 1/20 [00:00<00:10,  1.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Civet: 100%|██████████| 20/20 [00:14<00:00,  1.41it/s]\n",
            "Overall Progress:   6%|▋         | 8/127 [01:17<13:15,  6.69s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Whale:   5%|▌         | 1/20 [00:00<00:08,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Civet: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Pangolins: 100%|██████████| 20/20 [00:15<00:00,  1.28it/s]\n",
            "Overall Progress:   7%|▋         | 9/127 [01:21<11:17,  5.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Pangolins: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "Fetching images for Rabbit: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n",
            "Overall Progress:   8%|▊         | 10/127 [01:22<08:52,  4.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Rabbit: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Bats: 100%|██████████| 20/20 [00:13<00:00,  1.43it/s]\n",
            "Overall Progress:   9%|▊         | 11/127 [01:27<08:33,  4.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Bats: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for flamingo:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "Fetching images for Owl:   5%|▌         | 1/20 [00:02<00:47,  2.52s/it]\u001b[A\n",
            "\n",
            "Fetching images for Whale: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n",
            "Overall Progress:   9%|▉         | 12/127 [01:33<09:54,  5.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Whale: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Cock: 100%|██████████| 20/20 [00:17<00:00,  1.18it/s]\n",
            "Overall Progress:  10%|█         | 13/127 [01:38<09:11,  4.83s/it]\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Lizard:   5%|▌         | 1/20 [00:01<00:20,  1.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Cock: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "Fetching images for Turtle:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Owl: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
            "Overall Progress:  11%|█         | 14/127 [01:44<09:48,  5.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Owl: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for flamingo: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
            "Overall Progress:  12%|█▏        | 15/127 [01:46<08:03,  4.31s/it]\n",
            "Fetching images for Snake:   5%|▌         | 1/20 [00:01<00:21,  1.15s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for flamingo: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for Frog:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Fish:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Frog:   5%|▌         | 1/20 [00:00<00:08,  2.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Lizard: 100%|██████████| 20/20 [00:13<00:00,  1.47it/s]\n",
            "Overall Progress:  13%|█▎        | 16/127 [01:50<08:06,  4.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Lizard: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for shrimp:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Turtle: 100%|██████████| 20/20 [00:13<00:00,  1.48it/s]\n",
            "Overall Progress:  13%|█▎        | 17/127 [01:54<07:46,  4.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Turtle: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "Fetching images for Crab:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Snake: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n",
            "Overall Progress:  14%|█▍        | 18/127 [02:00<08:32,  4.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Snake: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Fish: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n",
            "Fetching images for Frog: 100%|██████████| 20/20 [00:14<00:00,  1.41it/s]\n",
            "Overall Progress:  16%|█▌        | 20/127 [02:02<04:53,  2.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Fish: 20\n",
            "\n",
            "Number of images retrieved for Frog: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for shrimp: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n",
            "Overall Progress:  17%|█▋        | 21/127 [02:09<06:59,  3.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for shrimp: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Snail:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for Snail:   5%|▌         | 1/20 [00:02<00:50,  2.67s/it]\u001b[A\n",
            "\n",
            "Fetching images for Coral:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Crab: 100%|██████████| 20/20 [00:18<00:00,  1.07it/s]\n",
            "Overall Progress:  17%|█▋        | 22/127 [02:16<08:30,  4.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Crab: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Jellyfish:   5%|▌         | 1/20 [00:01<00:25,  1.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Coral:   5%|▌         | 1/20 [00:03<00:57,  3.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Butterfly:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Butterfly:   5%|▌         | 1/20 [00:00<00:12,  1.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Flies:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Snail: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n",
            "Overall Progress:  18%|█▊        | 23/127 [02:26<11:03,  6.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Snail: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Jellyfish: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n",
            "Overall Progress:  19%|█▉        | 24/127 [02:33<11:08,  6.49s/it]\n",
            "Fetching images for Mosquito:   5%|▌         | 1/20 [00:01<00:31,  1.64s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Jellyfish: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Coral: 100%|██████████| 20/20 [00:18<00:00,  1.08it/s]\n",
            "Overall Progress:  20%|█▉        | 25/127 [02:33<08:05,  4.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Coral: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Butterfly: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n",
            "Overall Progress:  20%|██        | 26/127 [02:37<07:31,  4.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Butterfly: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Flies: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
            "Overall Progress:  21%|██▏       | 27/127 [02:38<05:54,  3.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Flies: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for Ants:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Ants:   5%|▌         | 1/20 [00:00<00:18,  1.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Cockroaches:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Cockroaches:   5%|▌         | 1/20 [00:00<00:17,  1.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Mosquito: 100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n",
            "Overall Progress:  22%|██▏       | 28/127 [02:49<09:12,  5.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Mosquito: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Spider:   5%|▌         | 1/20 [00:01<00:37,  1.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Fetching images for scorpion:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for scorpion:   5%|▌         | 1/20 [00:01<00:20,  1.09s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for tiger:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Ants: 100%|██████████| 20/20 [00:14<00:00,  1.39it/s]\n",
            "Overall Progress:  23%|██▎       | 29/127 [02:56<09:57,  6.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Ants: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Cockroaches: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n",
            "Overall Progress:  24%|██▎       | 30/127 [02:57<07:21,  4.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Cockroaches: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for bird:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for bird:   5%|▌         | 1/20 [00:02<00:38,  2.00s/it]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for bird:  10%|█         | 2/20 [00:02<00:16,  1.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Spider: 100%|██████████| 20/20 [00:18<00:00,  1.09it/s]\n",
            "Overall Progress:  24%|██▍       | 31/127 [03:06<09:26,  5.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Spider: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "Fetching images for horse:   5%|▌         | 1/20 [00:02<00:44,  2.33s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for scorpion: 100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\n",
            "Overall Progress:  25%|██▌       | 32/127 [03:08<07:13,  4.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for scorpion: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for tiger: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n",
            "Overall Progress:  26%|██▌       | 33/127 [03:08<05:23,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for tiger: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for pig:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for pig:   5%|▌         | 1/20 [00:00<00:11,  1.66it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Alligator:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Alligator:   5%|▌         | 1/20 [00:00<00:15,  1.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Alpaca:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for bird: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n",
            "Overall Progress:  27%|██▋       | 34/127 [03:16<07:26,  4.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for bird: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for horse: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n",
            "Overall Progress:  28%|██▊       | 35/127 [03:20<06:38,  4.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for horse: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for Anteater:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for pig: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n",
            "Overall Progress:  28%|██▊       | 36/127 [03:27<07:49,  5.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for pig: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Alligator: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n",
            "Overall Progress:  29%|██▉       | 37/127 [03:31<07:27,  4.98s/it]\n",
            "Fetching images for donkey:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Alligator: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Alpaca: 100%|██████████| 20/20 [00:18<00:00,  1.10it/s]\n",
            "Overall Progress:  30%|██▉       | 38/127 [03:33<05:59,  4.04s/it]\n",
            "Fetching images for donkey:   5%|▌         | 1/20 [00:02<00:38,  2.02s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Alpaca: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "Fetching images for Bee:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Anteater: 100%|██████████| 20/20 [00:18<00:00,  1.10it/s]\n",
            "Overall Progress:  31%|███       | 39/127 [03:39<06:56,  4.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Anteater: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for Buffalo:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Buffalo:   5%|▌         | 1/20 [00:00<00:09,  2.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Camel:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Camel:   5%|▌         | 1/20 [00:00<00:13,  1.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Bamboo:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for donkey: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n",
            "Overall Progress:  31%|███▏      | 40/127 [03:46<07:36,  5.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for donkey: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Apple:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for Bee: 100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n",
            "Overall Progress:  32%|███▏      | 41/127 [03:51<07:38,  5.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Bee: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Buffalo: 100%|██████████| 20/20 [00:15<00:00,  1.33it/s]\n",
            "Overall Progress:  33%|███▎      | 42/127 [03:56<07:07,  5.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Buffalo: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Camel: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
            "Overall Progress:  34%|███▍      | 43/127 [03:59<06:13,  4.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Camel: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for Bamboo: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n",
            "Overall Progress:  35%|███▍      | 44/127 [04:00<04:43,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Bamboo: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for Apple: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n",
            "Overall Progress:  35%|███▌      | 45/127 [04:05<05:33,  4.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Apple: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Banana:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Bean:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "Fetching images for Banana:   5%|▌         | 1/20 [00:01<00:23,  1.25s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Wildflower:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Bean:   5%|▌         | 1/20 [00:00<00:17,  1.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Wildflower:   5%|▌         | 1/20 [00:02<00:41,  2.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Flower:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Apricot: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n",
            "Overall Progress:  36%|███▌      | 46/127 [04:14<07:09,  5.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Apricot: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for Mushroom:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Banana: 100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n",
            "Overall Progress:  37%|███▋      | 47/127 [04:23<08:49,  6.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Banana: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Bean: 100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n",
            "Overall Progress:  38%|███▊      | 48/127 [04:25<06:56,  5.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Bean: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Wildflower: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n",
            "Fetching images for Flower: 100%|██████████| 20/20 [00:14<00:00,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Wildflower: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Overall Progress:  39%|███▉      | 50/127 [04:27<03:51,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Flower: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Shrub:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Mushroom: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
            "Overall Progress:  40%|████      | 51/127 [04:33<04:54,  3.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Mushroom: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for Fern:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Weed:   5%|▌         | 1/20 [00:05<01:37,  5.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Fern:   5%|▌         | 1/20 [00:02<00:46,  2.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Reed:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Fetching images for Shrub:   5%|▌         | 1/20 [00:10<03:20, 10.55s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Reed:   5%|▌         | 1/20 [00:01<00:29,  1.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Moss:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Weed: 100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
            "Overall Progress:  41%|████      | 52/127 [04:49<09:30,  7.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Weed: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Fern: 100%|██████████| 20/20 [00:15<00:00,  1.29it/s]\n",
            "Overall Progress:  42%|████▏     | 53/127 [04:52<07:23,  5.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Fern: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Shrub: 100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n",
            "Overall Progress:  43%|████▎     | 54/127 [04:55<06:11,  5.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Shrub: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Reed: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
            "Overall Progress:  43%|████▎     | 55/127 [04:56<04:50,  4.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Reed: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Moss: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n",
            "Overall Progress:  44%|████▍     | 56/127 [05:00<04:30,  3.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Moss: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Grass:   5%|▌         | 1/20 [00:02<00:48,  2.56s/it]\u001b[A\n",
            "\n",
            "Fetching images for Palmtree:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Palmtree:   5%|▌         | 1/20 [00:02<00:45,  2.39s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Corn:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Corn:   5%|▌         | 1/20 [00:00<00:10,  1.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Tulip:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Tulip:   5%|▌         | 1/20 [00:00<00:12,  1.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Rose:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Grass: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s]\n",
            "Overall Progress:  45%|████▍     | 57/127 [05:14<08:15,  7.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Grass: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Palmtree: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n",
            "Overall Progress:  46%|████▌     | 58/127 [05:18<07:05,  6.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Palmtree: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Corn: 100%|██████████| 20/20 [00:18<00:00,  1.06it/s]\n",
            "Fetching images for Tulip: 100%|██████████| 20/20 [00:18<00:00,  1.10it/s]\n",
            "Overall Progress:  46%|████▋     | 59/127 [05:24<06:41,  5.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Corn: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Rose: 100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n",
            "Overall Progress:  48%|████▊     | 61/127 [05:24<03:21,  3.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Tulip: 20\n",
            "\n",
            "Number of images retrieved for Rose: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Clove:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for Clove:   5%|▌         | 1/20 [00:01<00:29,  1.57s/it]\u001b[A\n",
            "\n",
            "Fetching images for Dogwood:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Dogwood:   5%|▌         | 1/20 [00:01<00:32,  1.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Durian:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Fig:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Ferns:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Durian:   5%|▌         | 1/20 [00:01<00:12,  1.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Durian:  10%|█         | 2/20 [00:01<00:12,  1.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Fig:   5%|▌         | 1/20 [00:01<00:29,  1.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Clove: 100%|██████████| 20/20 [00:14<00:00,  1.41it/s]\n",
            "Overall Progress:  49%|████▉     | 62/127 [05:40<07:25,  6.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Clove: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Flax:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for Dogwood: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n",
            "Overall Progress:  50%|████▉     | 63/127 [05:45<06:40,  6.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Dogwood: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Ferns: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
            "Overall Progress:  50%|█████     | 64/127 [05:50<06:23,  6.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Ferns: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Durian: 100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\n",
            "Fetching images for Fig: 100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
            "Overall Progress:  51%|█████     | 65/127 [05:51<04:37,  4.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Fig: 20\n",
            "\n",
            "Number of images retrieved for Durian: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for Frangipani:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Frangipani:   5%|▌         | 1/20 [00:01<00:24,  1.26s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Bougainvillea:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Lantana:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Flax: 100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
            "Overall Progress:  53%|█████▎    | 67/127 [06:01<04:44,  4.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Flax: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Lantana:   5%|▌         | 1/20 [00:01<00:26,  1.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Hibiscus:   5%|▌         | 1/20 [00:01<00:27,  1.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Bougainvillea:   5%|▌         | 1/20 [00:01<00:30,  1.63s/it]\u001b[A\u001b[A\u001b[A\n",
            "Fetching images for Pea:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for Frangipani: 100%|██████████| 20/20 [00:13<00:00,  1.47it/s]\n",
            "Overall Progress:  54%|█████▎    | 68/127 [06:05<04:30,  4.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Frangipani: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for OrchidTree:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Hibiscus: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n",
            "Fetching images for Lantana: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n",
            "Fetching images for Bougainvillea: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n",
            "Overall Progress:  56%|█████▌    | 71/127 [06:16<03:20,  3.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Hibiscus: 20\n",
            "\n",
            "Number of images retrieved for Lantana: 20\n",
            "\n",
            "Number of images retrieved for Bougainvillea: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Pea: 100%|██████████| 20/20 [00:13<00:00,  1.48it/s]\n",
            "Overall Progress:  57%|█████▋    | 72/127 [06:18<02:55,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Pea: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for OrchidTree: 100%|██████████| 20/20 [00:18<00:00,  1.09it/s]\n",
            "Overall Progress:  57%|█████▋    | 73/127 [06:27<04:01,  4.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for OrchidTree: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Cottonplant:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Fetching images for RangoonCreeper:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Cornelainetree:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Jackfruit:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Fetching images for Cottonplant:   5%|▌         | 1/20 [00:01<00:21,  1.16s/it]\u001b[A\n",
            "\n",
            "Fetching images for RangoonCreeper:   5%|▌         | 1/20 [00:01<00:25,  1.33s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Jackfruit:   5%|▌         | 1/20 [00:01<00:23,  1.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Coffeplant:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Cornelainetree:   0%|          | 0/20 [00:13<?, ?it/s]\n",
            "Overall Progress:  58%|█████▊    | 74/127 [06:42<06:35,  7.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Cornelainetree: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for RangoonCreeper: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s]\n",
            "Fetching images for Cottonplant: 100%|██████████| 20/20 [00:16<00:00,  1.25it/s]\n",
            "Overall Progress:  60%|█████▉    | 76/127 [06:44<03:33,  4.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for RangoonCreeper: 20\n",
            "\n",
            "Number of images retrieved for Cottonplant: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Jackfruit: 100%|██████████| 20/20 [00:15<00:00,  1.29it/s]\n",
            "Overall Progress:  61%|██████    | 77/127 [06:44<02:35,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Jackfruit: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Coffeplant: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n",
            "Overall Progress:  61%|██████▏   | 78/127 [06:47<02:23,  2.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Coffeplant: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Coconut:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Fetching images for watermelon:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "Fetching images for Coconut:   5%|▌         | 1/20 [00:01<00:18,  1.01it/s]\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for wheat:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for radish:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for wheat:   5%|▌         | 1/20 [00:01<00:20,  1.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for watermelon:   5%|▌         | 1/20 [00:01<00:24,  1.29s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for radish:   5%|▌         | 1/20 [00:01<00:24,  1.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for carrot:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Coconut: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n",
            "Overall Progress:  62%|██████▏   | 79/127 [07:07<06:20,  7.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Coconut: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for watermelon: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n",
            "Fetching images for wheat: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n",
            "Overall Progress:  63%|██████▎   | 80/127 [07:09<04:58,  6.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for watermelon: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rOverall Progress:  64%|██████▍   | 81/127 [07:09<03:28,  4.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for wheat: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for radish: 100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
            "Overall Progress:  65%|██████▍   | 82/127 [07:13<03:06,  4.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for radish: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for carrot: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n",
            "Overall Progress:  65%|██████▌   | 83/127 [07:14<02:25,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for carrot: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for bed:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for bed:   5%|▌         | 1/20 [00:01<00:29,  1.58s/it]\u001b[A\n",
            "\n",
            "Fetching images for chair:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for cabinet:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for chair:   5%|▌         | 1/20 [00:01<00:21,  1.11s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for cabinet:   5%|▌         | 1/20 [00:01<00:25,  1.33s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for chests:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for clock:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for chests:   5%|▌         | 1/20 [00:00<00:15,  1.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for bed: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n",
            "Overall Progress:  66%|██████▌   | 84/127 [07:31<05:15,  7.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for bed: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for cabinet: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n",
            "Overall Progress:  67%|██████▋   | 85/127 [07:37<04:56,  7.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for cabinet: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for desks:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for clock: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
            "Fetching images for chests: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n",
            "Overall Progress:  69%|██████▊   | 87/127 [07:41<02:51,  4.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for clock: 20\n",
            "\n",
            "Number of images retrieved for chests: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for chair: 100%|██████████| 20/20 [00:22<00:00,  1.13s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for table:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for table:   5%|▌         | 1/20 [00:01<00:19,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Bookcase:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Piano:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Piano:   5%|▌         | 1/20 [00:00<00:17,  1.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for desks: 100%|██████████| 20/20 [00:15<00:00,  1.29it/s]\n",
            "Overall Progress:  69%|██████▉   | 88/127 [07:53<04:15,  6.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for desks: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Umbrella:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for chair: 100%|██████████| 20/20 [00:36<00:00,  1.82s/it]\n",
            "Overall Progress:  70%|███████   | 89/127 [07:57<03:41,  5.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for chair: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for table: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s]\n",
            "Overall Progress:  71%|███████   | 90/127 [08:01<03:19,  5.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for table: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for Bookcase: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n",
            "Overall Progress:  72%|███████▏  | 91/127 [08:04<02:47,  4.64s/it]\n",
            "\n",
            "Fetching images for Piano: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Bookcase: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Overall Progress:  72%|███████▏  | 92/127 [08:05<01:56,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Piano: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "Fetching images for Umbrella: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
            "\n",
            "Overall Progress:  73%|███████▎  | 93/127 [08:13<02:44,  4.85s/it]\n",
            "\n",
            "\n",
            "Fetching images for cart:   5%|▌         | 1/20 [00:03<01:01,  3.25s/it]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Umbrella: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for ball:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Fetching images for sofa:   5%|▌         | 1/20 [00:02<00:45,  2.40s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Clothes: 100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n",
            "Overall Progress:  74%|███████▍  | 94/127 [08:18<02:38,  4.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Clothes: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for spoon:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for spoon:   5%|▌         | 1/20 [00:00<00:05,  3.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Bowl:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for cart: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n",
            "Overall Progress:  75%|███████▍  | 95/127 [08:27<03:19,  6.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for cart: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for ball: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n",
            "Fetching images for sofa: 100%|██████████| 20/20 [00:15<00:00,  1.25it/s]\n",
            "Overall Progress:  76%|███████▌  | 96/127 [08:29<02:30,  4.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for sofa: 20\n",
            "\n",
            "Number of images retrieved for ball: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for spoon: 100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n",
            "Overall Progress:  77%|███████▋  | 98/127 [08:32<01:37,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for spoon: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for pan:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Fetching images for Bowl: 100%|██████████| 20/20 [00:18<00:00,  1.08it/s]\n",
            "Overall Progress:  78%|███████▊  | 99/127 [08:39<01:58,  4.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Bowl: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for fridge:   5%|▌         | 1/20 [00:01<00:37,  1.97s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for book:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "Fetching images for pan:   5%|▌         | 1/20 [00:03<00:59,  3.13s/it]\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for book:   5%|▌         | 1/20 [00:01<00:30,  1.62s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Cliff:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Cliff:   5%|▌         | 1/20 [00:00<00:11,  1.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Bay:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for fridge: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n",
            "Overall Progress:  79%|███████▊  | 100/127 [08:55<03:15,  7.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for fridge: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for pan: 100%|██████████| 20/20 [00:18<00:00,  1.08it/s]\n",
            "Overall Progress:  80%|███████▉  | 101/127 [08:56<02:26,  5.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for pan: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for book: 100%|██████████| 20/20 [00:17<00:00,  1.11it/s]\n",
            "Overall Progress:  80%|████████  | 102/127 [08:58<01:55,  4.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for book: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Cliff: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n",
            "Overall Progress:  81%|████████  | 103/127 [08:59<01:22,  3.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Cliff: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Bay: 100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n",
            "Overall Progress:  82%|████████▏ | 104/127 [09:01<01:11,  3.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Bay: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Coast:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Fetching images for Mountains:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "Fetching images for Coast:   5%|▌         | 1/20 [00:02<00:50,  2.67s/it]\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Forests:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for Mountains:   5%|▌         | 1/20 [00:01<00:35,  1.88s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Waterbodies:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Lake:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Forests:   5%|▌         | 1/20 [00:01<00:22,  1.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Waterbodies:   5%|▌         | 1/20 [00:01<00:18,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for Coast: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n",
            "Overall Progress:  83%|████████▎ | 105/127 [09:21<03:00,  8.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Coast: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Mountains: 100%|██████████| 20/20 [00:17<00:00,  1.16it/s]\n",
            "Overall Progress:  83%|████████▎ | 106/127 [09:25<02:24,  6.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Mountains: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Forests: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n",
            "Fetching images for Waterbodies: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Forests: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for Lake: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Waterbodies: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Overall Progress:  86%|████████▌ | 109/127 [09:27<00:49,  2.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Lake: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for desert:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for desert:   5%|▌         | 1/20 [00:04<01:16,  4.04s/it]\u001b[A\n",
            "\n",
            "Fetching images for river:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Farmland:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for Farmland:   5%|▌         | 1/20 [00:00<00:12,  1.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for plain:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for river:   5%|▌         | 1/20 [00:01<00:19,  1.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for hedges:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for hedges:   5%|▌         | 1/20 [00:01<00:19,  1.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for desert: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
            "Overall Progress:  87%|████████▋ | 110/127 [09:48<02:19,  8.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for desert: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for Farmland: 100%|██████████| 20/20 [00:15<00:00,  1.25it/s]\n",
            "Overall Progress:  87%|████████▋ | 111/127 [09:52<01:51,  6.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for Farmland: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for river: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
            "Fetching images for hedges: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n",
            "Overall Progress:  89%|████████▉ | 113/127 [09:53<00:51,  3.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for river: 20\n",
            "\n",
            "Number of images retrieved for hedges: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for plain: 100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n",
            "Overall Progress:  90%|████████▉ | 114/127 [09:55<00:39,  3.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for plain: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for sky:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for sky:   5%|▌         | 1/20 [00:02<00:40,  2.12s/it]\u001b[A\n",
            "\n",
            "Fetching images for cave:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for cloud:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for flowergarden:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for cave:   5%|▌         | 1/20 [00:02<00:44,  2.35s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for glacier:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for flowergarden:   5%|▌         | 1/20 [00:02<00:50,  2.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for cloud:   5%|▌         | 1/20 [00:03<00:59,  3.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for sky: 100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n",
            "Overall Progress:  91%|█████████ | 115/127 [10:12<01:25,  7.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for sky: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for grassland:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for cave: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n",
            "Overall Progress:  91%|█████████▏| 116/127 [10:20<01:21,  7.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for cave: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for glacier: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n",
            "Fetching images for flowergarden: 100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
            "Fetching images for cloud: 100%|██████████| 20/20 [00:18<00:00,  1.11it/s]\n",
            "Overall Progress:  94%|█████████▎| 119/127 [10:22<00:25,  3.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for glacier: 20\n",
            "\n",
            "Number of images retrieved for flowergarden: 20\n",
            "\n",
            "Number of images retrieved for cloud: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for horizon:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for horizon:   5%|▌         | 1/20 [00:00<00:10,  1.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for plateau:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for lighthouse:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching images for plateau:   5%|▌         | 1/20 [00:00<00:15,  1.25it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for grassland: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Overall Progress:  94%|█████████▍| 120/127 [10:30<00:31,  4.53s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching images for savannah:   5%|▌         | 1/20 [00:01<00:30,  1.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for grassland: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for valley:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for horizon: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n",
            "Overall Progress:  95%|█████████▌| 121/127 [10:38<00:31,  5.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for horizon: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for plateau: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n",
            "Overall Progress:  96%|█████████▌| 122/127 [10:44<00:27,  5.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for plateau: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Fetching images for volcano:   5%|▌         | 1/20 [00:01<00:29,  1.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "Fetching images for lighthouse: 100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\n",
            "Fetching images for savannah: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n",
            "Overall Progress:  98%|█████████▊| 124/127 [10:46<00:10,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for lighthouse: 20\n",
            "\n",
            "Number of images retrieved for savannah: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for valley: 100%|██████████| 20/20 [00:15<00:00,  1.33it/s]\n",
            "Overall Progress:  98%|█████████▊| 125/127 [10:47<00:05,  2.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for valley: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fetching images for waterfall:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching images for volcano: 100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n",
            "Overall Progress:  99%|█████████▉| 126/127 [10:58<00:05,  5.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for volcano: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching images for waterfall: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s]\n",
            "Overall Progress: 100%|██████████| 127/127 [11:02<00:00,  5.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of images retrieved for waterfall: 20\n",
            "Data saved to image_urls.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thu thập dữ liệu - Crawl ảnh từ URL**"
      ],
      "metadata": {
        "id": "aQIRj0uTTj5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import urllib.request\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "class ImageDownloader:\n",
        "    def __init__(self, json_file, download_dir='Dataset', max_workers=4, delay=1):\n",
        "        self.json_file = json_file  # File chứa các URL hình ảnh ở định dạng JSON\n",
        "        self.download_dir = download_dir  # Tên thư mục để lưu trữ hình ảnh\n",
        "        self.max_workers = max_workers  # Số luồng thực thi\n",
        "        self.delay = delay  # Độ trễ giữa các request để tránh việc gửi quá nhiều yêu cầu tới máy chủ\n",
        "        self.filename = set()  # Để lưu trữ các đường dẫn file đã tải xuống\n",
        "        self.setup_directory()  # Thiết lập cấu trúc thư mục\n",
        "\n",
        "    def setup_directory(self):\n",
        "        if not os.path.exists(self.download_dir):\n",
        "            os.makedirs(self.download_dir)\n",
        "\n",
        "    def read_json(self):\n",
        "        \"\"\"\n",
        "        Đọc file JSON và trả về dữ liệu.\n",
        "\n",
        "        Returns:\n",
        "        data (dict): Dữ liệu được đọc từ file JSON.\n",
        "        \"\"\"\n",
        "        with open(self.json_file, 'r') as file:\n",
        "            data = json.load(file)\n",
        "        return data\n",
        "\n",
        "    def is_valid_url(self, url):\n",
        "        \"\"\"\n",
        "        Kiểm tra xem URL có hợp lệ hay không.\n",
        "\n",
        "        Parameters:\n",
        "        url (str): URL cần kiểm tra.\n",
        "\n",
        "        Returns:\n",
        "        bool: True nếu URL hợp lệ, False nếu không hợp lệ.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with urllib.request.urlopen(url) as response:\n",
        "                if response.status == 200 and 'image' in response.info().get_content_type():\n",
        "                    return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def download_image(self, url, category, term, pbar):\n",
        "        \"\"\"\n",
        "        Tải hình ảnh từ URL đã cho.\n",
        "\n",
        "        Parameters:\n",
        "        url (str): URL của hình ảnh cần tải xuống.\n",
        "        category (str): Danh mục của hình ảnh.\n",
        "        term (str): Từ khóa hoặc thuật ngữ liên quan đến hình ảnh.\n",
        "        pbar (tqdm): Đối tượng thanh tiến trình.\n",
        "\n",
        "        Returns:\n",
        "        str: Thông báo về trạng thái của quá trình tải xuống.\n",
        "        \"\"\"\n",
        "        if not self.is_valid_url(url):\n",
        "            pbar.update(1)\n",
        "            return f\"Invalid URL: {url}\"\n",
        "\n",
        "        category_dir = os.path.join(self.download_dir, category)\n",
        "        if not os.path.exists(category_dir):\n",
        "            os.makedirs(category_dir)\n",
        "\n",
        "        term_dir = os.path.join(category_dir, term)\n",
        "        if not os.path.exists(term_dir):\n",
        "            os.makedirs(term_dir)\n",
        "\n",
        "        filename = os.path.join(term_dir, os.path.basename(urlparse(url).path))\n",
        "        self.filename.add(filename)  # Ghi lại đường dẫn file đã tải xuống\n",
        "\n",
        "        try:\n",
        "            urllib.request.urlretrieve(url, filename)\n",
        "            pbar.update(1)\n",
        "            return f\"Downloaded: {url}\"\n",
        "        except Exception as e:\n",
        "            pbar.update(1)\n",
        "            return f\"Failed to download {url}: {str(e)}\"\n",
        "\n",
        "    def download_images(self):\n",
        "        \"\"\"\n",
        "        Tải xuống các hình ảnh từ file JSON.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        data = self.read_json()\n",
        "        download_tasks = []\n",
        "\n",
        "        total_images = sum(len(urls) for terms in data.values() for urls in terms.values())\n",
        "        with tqdm(total=total_images, desc=\"Downloading images\") as pbar:\n",
        "            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "                for category, terms in data.items():\n",
        "                    for term, urls in terms.items():\n",
        "                        for url in urls:\n",
        "                            download_tasks.append(\n",
        "                                executor.submit(self.download_image, url, category, term, pbar)\n",
        "                            )\n",
        "                        time.sleep(self.delay)  # Thêm độ trễ để tránh gửi quá nhiều yêu cầu tới máy chủ\n",
        "\n",
        "            for future in concurrent.futures.as_completed(download_tasks):\n",
        "                print(future.result())\n",
        "\n",
        "        self.export_filename()\n",
        "\n",
        "    def export_filename(self):\n",
        "        \"\"\"\n",
        "        Xuất các đường dẫn file đã tải xuống vào một file văn bản.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        with open('filename.txt', 'w') as file:\n",
        "            for filename in sorted(self.filename):\n",
        "                file.write(f\"{filename}\\n\")"
      ],
      "metadata": {
        "id": "-Zl7FR7xOJ4b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "downloader = ImageDownloader(json_file='image_urls.json', download_dir='Dataset', max_workers=4, delay=1)\n",
        "downloader.download_images()\n",
        "downloader.export_filename()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "KmILo2HrTRGT",
        "outputId": "97943d14-68ac-4d39-a775-0ed880ba50f2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading images:  17%|█▋        | 420/2520 [00:34<02:51, 12.21it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-77cc318ff179>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdownloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'image_urls.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-aa7cc795cb93>\u001b[0m in \u001b[0;36mdownload_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m                                 \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                             )\n\u001b[0;32m--> 106\u001b[0;31m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Thêm độ trễ để tránh gửi quá nhiều yêu cầu tới máy chủ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Xử lí dữ liệu - Làm sạch bộ dữ liệu**"
      ],
      "metadata": {
        "id": "sKU4hV_4T2_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('content/drive/')"
      ],
      "metadata": {
        "id": "YrMKLJeMTfQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_and_preprocess_images(image_dir):\n",
        "  \"\"\"\n",
        "  Check and preprocess images in the specified directory.\n",
        "\n",
        "  Parameters:\n",
        "  image_dir (str): The directory containing the images to be checked and preprocessed.\n",
        "\n",
        "  Returns:\n",
        "\n",
        "  None\n",
        "  \"\"\"\n",
        "  for root, _, files in os.walk(image_dir):\n",
        "    for filename in files:\n",
        "      filepath = os.path.join(root, file)\n",
        "      try:\n",
        "        with Image.open(filepath) as img:\n",
        "          #Check if image is smaller than 50x50 pixels\n",
        "          if img.size[0] < 50 or img.size[1] < 50:\n",
        "            os.remove(filepath)\n",
        "            print(f\"Deleted {file_path}: Image too small ({img.size[0]}x{img.size[1]})\")\n",
        "            continue\n",
        "\n",
        "          #Convert non-RGB images to RGB\n",
        "          if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "            img.save(filepath)\n",
        "            print(f\"Converted {file_path} to RGB\")\n",
        "\n",
        "      except Exception as e:\n",
        "        #If file is not an image, delete it\n",
        "        os.remove(filepath)\n",
        "        print(f\"Deleted {file_path}: Not an image or corrupted file ({str(e)})\")\n",
        "\n",
        "\n",
        "check_and_preprocess_images('Dataset')"
      ],
      "metadata": {
        "id": "DLlhEoRCUcyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/MyDrive/Clean_Dataset.zip Dataset"
      ],
      "metadata": {
        "id": "v8KwRdMAVa-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Xử lí dữ liệu - Tổ chức cấu trúc folder**"
      ],
      "metadata": {
        "id": "zWnZlG5tVhcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1--6fe48D9ydnTpLV1GKKqJ0pqpOXB3z_"
      ],
      "metadata": {
        "id": "m6z4KUQVVjAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Clean_Dataset"
      ],
      "metadata": {
        "id": "B95f6E6dVrTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define the source and target directories\n",
        "source_dir = \"Dataset\"\n",
        "train_dir = \"data/train\"\n",
        "test_dir = \"data/test\"\n",
        "\n",
        "# Create the target directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Initialize a dictionary to hold file paths for each class\n",
        "class_files = defaultdict(list)\n",
        "\n",
        "# Read the file paths from the text file\n",
        "with open('filename.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            # Extract the class name from the path\n",
        "            parts = line.split('/')\n",
        "            class_name = parts[2]  # Structure Dataset/category/class/image.jpg\n",
        "            class_files[class_name].append(line)\n",
        "\n",
        "# Move images to the train and test directories\n",
        "for class_name, files in class_files.items():\n",
        "    # Create the train and test directories for the class\n",
        "    train_class_dir = os.path.join(train_dir, class_name)\n",
        "    test_class_dir = os.path.join(test_dir, class_name)\n",
        "    os.makedirs(train_class_dir, exist_ok=True)\n",
        "    os.makedirs(test_class_dir, exist_ok=True)\n",
        "\n",
        "    # Move 19 images to train and 1 image to test\n",
        "    for i, file_path in enumerate(files):\n",
        "        if i == 0:\n",
        "            shutil.copy(file_path, test_class_dir)\n",
        "        elif i < 20:\n",
        "            shutil.copy(file_path, train_class_dir)\n",
        "\n",
        "print(\"Dataset organization complete!\")"
      ],
      "metadata": {
        "id": "FZ3s1Ev3Vw5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/MyDrive/data.zip data"
      ],
      "metadata": {
        "id": "onk-xbyPWLC1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}